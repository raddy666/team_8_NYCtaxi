{
  "metadata": {
    "name": "yellow_taxi",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\n\r\nfrom pyspark.sql.session import SparkSession\r\nfrom pyspark.sql.functions import col, concat, lit, to_timestamp, year, avg, month, count, weekofyear, rand, when\r\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType, LongType, BooleanType, FloatType,DoubleType\r\n\r\nspark \u003d SparkSession.builder\\\r\n    .appName(\u0027yellow_taxi_filter\u0027)\\\r\n    .master(\u0027local\u0027)\\\r\n    .getOrCreate()\r\n\r\nschema \u003d StructType([\r\n    StructField(\"vendor_name\", StringType(), True),\r\n    StructField(\"Trip_Pickup_DateTime\", StringType(), True),\r\n    StructField(\"Trip_Dropoff_DateTime\", StringType(), True),\r\n    StructField(\"Passenger_Count\", StringType(), True),\r\n    StructField(\"Trip_Distance\", StringType(), True),\r\n    StructField(\"Start_Lon\", StringType(), True),\r\n    StructField(\"Start_Lat\", StringType(), True),\r\n    StructField(\"Rate_Code\", StringType(), True),\r\n    StructField(\"store_and_forward\", StringType(), True),\r\n    StructField(\"End_Lon\", StringType(), True),\r\n    StructField(\"End_Lat\", StringType(), True),\r\n    StructField(\"Payment_Type\", StringType(), True),\r\n    StructField(\"Fare_Amt\", StringType(), True),\r\n    StructField(\"surcharge\", StringType(), True),\r\n    StructField(\"mta_tax\", StringType(), True),\r\n    StructField(\"Tip_Amt\", StringType(), True),\r\n    StructField(\"Tolls_Amt\", StringType(), True),\r\n    StructField(\"Total_Amt\", StringType(), True)\r\n])\r\n#\"hdfs://nodemastertah:9000/NYCtaxi/dataset/yellow_taxi/yellow_csv/*.csv\"\r\n# Read the CSV file with all fields as strings\r\ndf \u003d spark.read.csv(\"hdfs://nodemastertah:9000/NYCtaxi/dataset/yellow_taxi/yellow_csv/*.csv\", header\u003dTrue, schema\u003dschema)\r\n\r\n\r\ndf \u003d df.withColumn(\"Trip_Pickup_DateTime\", col(\"Trip_Pickup_DateTime\").cast(\"timestamp\")) \\\r\n       .withColumn(\"Trip_Dropoff_DateTime\", col(\"Trip_Dropoff_DateTime\").cast(\"timestamp\")) \\\r\n       .withColumn(\"Passenger_Count\", col(\"Passenger_Count\").cast(\"integer\")) \\\r\n       .withColumn(\"Trip_Distance\", col(\"Trip_Distance\").cast(\"double\")) \\\r\n       .withColumn(\"Start_Lon\", col(\"Start_Lon\").cast(\"double\")) \\\r\n       .withColumn(\"Start_Lat\", col(\"Start_Lat\").cast(\"double\")) \\\r\n       .withColumn(\"Rate_Code\", col(\"Rate_Code\").cast(\"integer\")) \\\r\n       .withColumn(\"End_Lon\", col(\"End_Lon\").cast(\"double\")) \\\r\n       .withColumn(\"End_Lat\", col(\"End_Lat\").cast(\"double\")) \\\r\n       .withColumn(\"Fare_Amt\", col(\"Fare_Amt\").cast(\"double\")) \\\r\n       .withColumn(\"surcharge\", col(\"surcharge\").cast(\"double\")) \\\r\n       .withColumn(\"mta_tax\", col(\"mta_tax\").cast(\"double\")) \\\r\n       .withColumn(\"Tip_Amt\", col(\"Tip_Amt\").cast(\"double\")) \\\r\n       .withColumn(\"Tolls_Amt\", col(\"Tolls_Amt\").cast(\"double\")) \\\r\n       .withColumn(\"Total_Amt\", col(\"Total_Amt\").cast(\"double\"))\r\n\r\ndf.printSchema()\r\n\r\n\r\nmin_value \u003d 1\r\nmax_value \u003d 6\r\n\r\ndf_filled \u003d (df.withColumn(\r\n    \"Rate_Code\",\r\n    when(\r\n        col(\"Rate_Code\").isNull(),\r\n        (rand() * (max_value - min_value + 1) + min_value).cast(\"integer\")\r\n    ).otherwise(col(\"Rate_Code\"))\r\n).withColumn(\r\n    \"store_and_forward\",\r\n    when(df[\"store_and_forward\"].isNull() | (df[\"store_and_forward\"] !\u003d \u0027Y\u0027), \u0027N\u0027)\r\n    .otherwise(\u0027Y\u0027)\r\n).withColumn(\"mta_tax\", lit(0.5)))\r\n\r\n# Filter out rows with invalid latitude and longitude values\r\ndf_cleaned \u003d df_filled.filter(\r\n    (col(\"Start_Lat\").isNotNull()) \u0026\r\n    (col(\"End_Lat\").isNotNull()) \u0026\r\n    (col(\"Start_Lon\").isNotNull()) \u0026\r\n    (col(\"End_Lon\").isNotNull()) \u0026\r\n    (col(\"Start_Lat\") \u003e\u003d -90) \u0026 (col(\"Start_Lat\") \u003c\u003d 90) \u0026\r\n    (col(\"End_Lat\") \u003e\u003d -90) \u0026 (col(\"End_Lat\") \u003c\u003d 90) \u0026\r\n    (col(\"Start_Lon\") \u003e\u003d -180) \u0026 (col(\"Start_Lon\") \u003c\u003d 180) \u0026\r\n    (col(\"End_Lon\") \u003e\u003d -180) \u0026 (col(\"End_Lon\") \u003c\u003d 180)\r\n)\r\n\r\nprint(f\"Number of rows after removing invalid lat/lon: {df_cleaned.count()}\")\r\ndf_cleaned.show(5)\r\n\r\n\r\n# Remove rows where Trip_Distance is 0.00\r\ndf_cleaned \u003d df_filled.filter(col(\"Trip_Distance\") \u003e 0)\r\n\r\nprint(f\"Number of rows after removing zero-distance trips: {df_cleaned.count()}\")\r\ndf_cleaned.show(5)\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\nfrom pyspark.sql.functions import count\r\n\r\nlocation_heatmap \u003d df_cleaned.groupBy(\"Start_Lon\", \"Start_Lat\", \"End_Lon\", \"End_Lat\") \\\r\n    .agg(count(\"*\").alias(\"trip_count\")) \\\r\n    .orderBy(\"trip_count\", ascending\u003dFalse)\r\n\r\nz.show(location_heatmap)\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\nfrom pyspark.sql.functions import sum\r\n\r\npayment_revenue \u003d df_cleaned.groupBy(\"Payment_Type\") \\\r\n    .agg(sum(\"Total_Amt\").alias(\"total_revenue\")) \\\r\n    .orderBy(\"total_revenue\", ascending\u003dFalse)\r\n\r\nz.show(payment_revenue)\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\nfrom pyspark.sql.functions import hour\r\n\r\nhourly_demand \u003d df_cleaned.withColumn(\"hour\", hour(\"Trip_Pickup_DateTime\")) \\\r\n    .groupBy(\"hour\") \\\r\n    .agg(count(\"*\").alias(\"trip_count\")) \\\r\n    .orderBy(\"hour\")\r\n\r\nz.show(hourly_demand)\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\nz.show(df_cleaned.select(\"Trip_Distance\", \"Total_Amt\"))"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\nfraud_trips \u003d df_cleaned.filter((df_cleaned.Trip_Distance \u003c 0.5) \u0026 (df_cleaned.Total_Amt \u003e 50)) \\\r\n    .select(\"Trip_Distance\", \"Total_Amt\", \"Start_Lon\", \"Start_Lat\", \"End_Lon\", \"End_Lat\")\r\n\r\nz.show(fraud_trips)\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\nfrom pyspark.sql.functions import avg\r\n\r\ntip_heatmap \u003d df_cleaned.groupBy(\"Start_Lon\", \"Start_Lat\") \\\r\n    .agg(avg(\"Tip_Amt\").alias(\"avg_tip\")) \\\r\n    .orderBy(\"avg_tip\", ascending\u003dFalse)\r\n\r\nz.show(tip_heatmap)\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\nfrom pyspark.sql.functions import hour\r\n\r\nfare_demand \u003d df_cleaned.withColumn(\"hour\", hour(\"Trip_Pickup_DateTime\")) \\\r\n    .groupBy(\"hour\") \\\r\n    .agg(\r\n        count(\"*\").alias(\"trip_count\"),\r\n        avg(\"Total_Amt\").alias(\"avg_fare\")\r\n    ) \\\r\n    .orderBy(\"hour\")\r\n\r\nz.show(fare_demand)\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\nfrom pyspark.sql.functions import when\r\n\r\ntoll_impact \u003d df_cleaned.withColumn(\"Toll_Category\", when(df_cleaned.Tolls_Amt \u003e 0, \"With Toll\").otherwise(\"No Toll\")) \\\r\n    .groupBy(\"Toll_Category\") \\\r\n    .agg(avg(\"Total_Amt\").alias(\"avg_fare\"))\r\n\r\nz.show(toll_impact)\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\nfrom pyspark.sql.functions import avg, when\r\n\r\n# Function to tag landmarks based on pickup location\r\ndef add_landmark_column(df):\r\n    return df.withColumn(\r\n        \"Landmark\",\r\n        when(\r\n            (col(\"Start_Lat\").between(40.7550, 40.7590)) \u0026 (col(\"Start_Lon\").between(-73.9900, -73.9840)), \"Times Square\"\r\n        ).when(\r\n            (col(\"Start_Lat\").between(40.6390, 40.6500)) \u0026 (col(\"Start_Lon\").between(-73.7940, -73.7780)), \"JFK Airport\"\r\n        ).when(\r\n            (col(\"Start_Lat\").between(40.7660, 40.7770)) \u0026 (col(\"Start_Lon\").between(-73.8900, -73.8700)), \"LaGuardia Airport\"\r\n        ).when(\r\n            (col(\"Start_Lat\").between(40.7640, 40.8000)) \u0026 (col(\"Start_Lon\").between(-73.9810, -73.9490)), \"Central Park\"\r\n        ).otherwise(\"Other\")\r\n    )\r\n\r\n# Add landmark column to the DataFrame\r\ndf_with_landmark \u003d add_landmark_column(df_cleaned)\r\n\r\n# 🔹 Drop duplicate locations to prevent one landmark from dominating\r\ndf_unique_landmarks \u003d df_with_landmark.dropDuplicates([\"Start_Lon\", \"Start_Lat\", \"Landmark\"])\r\n\r\n# Compute average tip amount per pickup location \u0026 landmark\r\ntip_heatmap \u003d df_unique_landmarks.groupBy(\"Start_Lon\", \"Start_Lat\", \"Landmark\") \\\r\n    .agg(avg(\"Tip_Amt\").alias(\"avg_tip\")) \\\r\n    .orderBy(\"avg_tip\", ascending\u003dFalse)\r\n\r\n# Show top 200 records in Zeppelin\r\nz.show(tip_heatmap)\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\nfrom pyspark.sql.functions import avg, when\r\n\r\n# Function to tag landmarks based on pickup location\r\ndef add_landmark_column(df):\r\n    return df.withColumn(\r\n        \"Landmark\",\r\n        when(\r\n            (col(\"Start_Lat\").between(40.7550, 40.7590)) \u0026 (col(\"Start_Lon\").between(-73.9900, -73.9840)), \"Times Square\"\r\n        ).when(\r\n            (col(\"Start_Lat\").between(40.6390, 40.6500)) \u0026 (col(\"Start_Lon\").between(-73.7940, -73.7780)), \"JFK Airport\"\r\n        ).when(\r\n            (col(\"Start_Lat\").between(40.7660, 40.7770)) \u0026 (col(\"Start_Lon\").between(-73.8900, -73.8700)), \"LaGuardia Airport\"\r\n        ).when(\r\n            (col(\"Start_Lat\").between(40.7640, 40.8000)) \u0026 (col(\"Start_Lon\").between(-73.9810, -73.9490)), \"Central Park\"\r\n        ).otherwise(\"Other\")\r\n    )\r\n\r\n# Add landmark column to the DataFrame\r\ndf_with_landmark \u003d add_landmark_column(df_cleaned)\r\n\r\n# Compute average tip amount per pickup location \u0026 landmark\r\ntip_heatmap \u003d df_with_landmark.groupBy(\"Start_Lon\", \"Start_Lat\", \"Landmark\") \\\r\n    .agg(avg(\"Tip_Amt\").alias(\"avg_tip\")) \\\r\n    .orderBy(\"avg_tip\", ascending\u003dFalse)\r\n\r\n# Show top 200 records in Zeppelin\r\nz.show(tip_heatmap)\r\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}