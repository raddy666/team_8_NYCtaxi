{
  "metadata": {
    "name": "fhv",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\nfrom pyspark.sql.session import SparkSession\r\nfrom pyspark.sql.functions import col, concat, lit, to_timestamp, year, avg, month, count, weekofyear, rand, when\r\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType, LongType, BooleanType, FloatType, \\\r\n    DoubleType, TimestampType\r\n\r\nspark \u003d SparkSession.builder \\\r\n    .appName(\"fhv_data_processing\") \\\r\n    .master(\"local\") \\\r\n    .getOrCreate()\r\n\r\nschema_fhv \u003d StructType([\r\n    StructField(\"dispatching_base_num\", StringType(), True),\r\n    StructField(\"pickup_datetime\", TimestampType(), True),\r\n    StructField(\"dropOff_datetime\", TimestampType(), True),\r\n    StructField(\"PUlocationID\", DoubleType(), True),\r\n    StructField(\"DOlocationID\", DoubleType(), True),\r\n    StructField(\"SR_Flag\", IntegerType(), True),\r\n    StructField(\"Affiliated_base_number\", StringType(), True)\r\n])\r\n\r\n\r\ndf_fhv \u003d spark.read.csv(\"hdfs://nodemastertah:9000/NYCtaxi/dataset/fhv/fhv_csv/*.csv\", header\u003dTrue, schema\u003dschema_fhv)\r\n\r\ndf_fhv.show(5)\r\n\r\ndf_fill \u003d df_fhv.fillna({\"SR_Flag\": 0})\r\n\r\ndf_cleaned \u003d df_fill.dropna(subset\u003d[\"pickup_datetime\", \"dropOff_datetime\", \"PUlocationID\", \"DOlocationID\"]) \\\r\n       .filter((col(\"PUlocationID\") \u003e 0) \u0026 (col(\"DOlocationID\") \u003e 0))\r\n\r\ndf_cleaned.show(5)\r\n\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\nfrom pyspark.sql.functions import hour, count\r\n\r\npeak_hours \u003d df_cleaned.withColumn(\"hour\", hour(\"pickup_datetime\")) \\\r\n                       .groupBy(\"hour\") \\\r\n                       .agg(count(\"*\").alias(\"trip_count\")) \\\r\n                       .orderBy(\"hour\")\r\n\r\nz.show(peak_hours)\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\nfrom pyspark.sql.functions import col, count\r\n\r\nlocation_heatmap \u003d df_cleaned.groupBy(\"PUlocationID\", \"DOlocationID\") \\\r\n                             .agg(count(\"*\").alias(\"trip_count\")) \\\r\n                             .orderBy(col(\"trip_count\").desc())\r\n\r\nz.show(location_heatmap)\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\nfrom pyspark.sql.functions import unix_timestamp, avg\r\n\r\ntrip_duration \u003d df_cleaned.withColumn(\"duration\", \r\n                 (unix_timestamp(\"dropOff_datetime\") - unix_timestamp(\"pickup_datetime\")) / 60) \\\r\n             .groupBy(\"dispatching_base_num\") \\\r\n             .agg(avg(\"duration\").alias(\"avg_trip_duration\")) \\\r\n             .orderBy(col(\"avg_trip_duration\").desc())\r\n\r\nz.show(trip_duration)\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql.functions import hour\n\nhourly_trips \u003d df_cleaned.withColumn(\"hour\", hour(\"pickup_datetime\")) \\\n                         .groupBy(\"dispatching_base_num\", \"hour\") \\\n                         .count() \\\n                         .orderBy(\"dispatching_base_num\", \"hour\")\n\nz.show(hourly_trips)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\nfrom pyspark.sql.functions import hour, count\r\n\r\nlate_night_trips \u003d df_cleaned.withColumn(\"hour\", hour(\"pickup_datetime\")) \\\r\n                             .filter(\"hour \u003e\u003d 22 OR hour \u003c\u003d 4\") \\\r\n                             .groupBy(\"PUlocationID\", \"DOlocationID\") \\\r\n                             .agg(count(\"*\").alias(\"trip_count\")) \\\r\n                             .orderBy(col(\"trip_count\").desc())\r\n\r\nz.show(late_night_trips)\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\nfrom pyspark.sql.functions import dayofweek\r\n\r\nweekday_vs_weekend \u003d df_cleaned.withColumn(\"day_of_week\", dayofweek(\"pickup_datetime\")) \\\r\n                               .withColumn(\"day_type\", \r\n                                   when(col(\"day_of_week\").isin([1,7]), \"Weekend\").otherwise(\"Weekday\")) \\\r\n                               .groupBy(\"day_type\") \\\r\n                               .agg(count(\"*\").alias(\"trip_count\"))\r\n\r\nz.show(weekday_vs_weekend)\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\nfrom pyspark.sql.functions import count\r\n\r\nghost_bases \u003d df_cleaned.groupBy(\"dispatching_base_num\") \\\r\n                        .agg(count(\"*\").alias(\"trip_count\")) \\\r\n                        .filter(\"trip_count \u003c 100\") \\\r\n                        .orderBy(\"trip_count\")\r\n\r\nz.show(ghost_bases)\r\n"
    }
  ]
}