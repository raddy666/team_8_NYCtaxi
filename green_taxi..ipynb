{
  "metadata": {
    "name": "green_taxi",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\n\r\nfrom pyspark.sql.session import SparkSession\r\nfrom pyspark.sql.functions import col, concat, lit, to_timestamp, year, avg, month, count, weekofyear, rand, when\r\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType, LongType, BooleanType, FloatType,DoubleType\r\n\r\nspark \u003d SparkSession.builder\\\r\n    .appName(\u0027green_taxi_filter\u0027)\\\r\n    .master(\u0027local\u0027)\\\r\n    .getOrCreate()\r\n\r\nschema \u003d StructType([\r\n    StructField(\"VendorID\", StringType(), True),\r\n    StructField(\"lpep_pickup_datetime\", StringType(), True),\r\n    StructField(\"lpep_dropoff_datetime\", StringType(), True),\r\n    StructField(\"store_and_fwd_flag\", StringType(), True),\r\n    StructField(\"RatecodeID\", StringType(), True),\r\n    StructField(\"PULocationID\", StringType(), True),\r\n    StructField(\"DOLocationID\", StringType(), True),\r\n    StructField(\"passenger_count\", StringType(), True),\r\n    StructField(\"trip_distance\", StringType(), True),\r\n    StructField(\"fare_amount\", StringType(), True),\r\n    StructField(\"extra\", StringType(), True),\r\n    StructField(\"mta_tax\", StringType(), True),\r\n    StructField(\"tip_amount\", StringType(), True),\r\n    StructField(\"tolls_amount\", StringType(), True),\r\n    StructField(\"ehail_fee\", StringType(), True),\r\n    StructField(\"improvement_surcharge\", StringType(), True),\r\n    StructField(\"total_amount\", StringType(), True),\r\n    StructField(\"payment_type\", StringType(), True),\r\n    StructField(\"trip_type\", StringType(), True),\r\n    StructField(\"congestion_surcharge\", StringType(), True)\r\n])\r\n\r\n\r\ndf \u003d spark.read.csv(\"hdfs://nodemastertah:9000/NYCtaxi/dataset/green_taxi/green_csv/*.csv\", header\u003dTrue, schema\u003dschema)\r\n\r\n\r\ndf \u003d df.withColumn(\"VendorID\", col(\"VendorID\").cast(\"integer\")) \\\r\n       .withColumn(\"lpep_pickup_datetime\", col(\"lpep_pickup_datetime\").cast(\"timestamp\")) \\\r\n       .withColumn(\"lpep_dropoff_datetime\", col(\"lpep_dropoff_datetime\").cast(\"timestamp\")) \\\r\n       .withColumn(\"store_and_fwd_flag\", col(\"store_and_fwd_flag\").cast(\"string\")) \\\r\n       .withColumn(\"RatecodeID\", col(\"RatecodeID\").cast(\"integer\")) \\\r\n       .withColumn(\"PULocationID\", col(\"PULocationID\").cast(\"double\")) \\\r\n       .withColumn(\"DOLocationID\", col(\"DOLocationID\").cast(\"double\")) \\\r\n       .withColumn(\"passenger_count\", col(\"passenger_count\").cast(\"integer\")) \\\r\n       .withColumn(\"trip_distance\", col(\"trip_distance\").cast(\"double\")) \\\r\n       .withColumn(\"fare_amount\", col(\"fare_amount\").cast(\"double\")) \\\r\n       .withColumn(\"extra\", col(\"extra\").cast(\"double\")) \\\r\n       .withColumn(\"mta_tax\", col(\"mta_tax\").cast(\"double\")) \\\r\n       .withColumn(\"tip_amount\", col(\"tip_amount\").cast(\"double\")) \\\r\n       .withColumn(\"tolls_amount\", col(\"tolls_amount\").cast(\"double\")) \\\r\n       .withColumn(\"ehail_fee\", col(\"ehail_fee\").cast(\"double\")) \\\r\n       .withColumn(\"improvement_surcharge\", col(\"improvement_surcharge\").cast(\"double\")) \\\r\n       .withColumn(\"total_amount\", col(\"total_amount\").cast(\"double\")) \\\r\n       .withColumn(\"payment_type\", col(\"payment_type\").cast(\"integer\")) \\\r\n       .withColumn(\"trip_type\", col(\"trip_type\").cast(\"integer\")) \\\r\n       .withColumn(\"congestion_surcharge\", col(\"congestion_surcharge\").cast(\"double\"))\r\n\r\ndf.printSchema()\r\n\r\ndf_cleaned \u003d df.filter(\r\n    (col(\"passenger_count\") \u003e\u003d 1) \u0026 (col(\"passenger_count\") \u003c\u003d 10) \u0026  \r\n    (col(\"payment_type\").isNotNull())  \r\n)\r\n\r\ndf_cleaned \u003d df_cleaned.withColumn(\"ehail_fee\", lit(0))\r\n\r\nlegal_rows_count \u003d df_cleaned.count()\r\n\r\nprint(f\"clean data:\")\r\n\r\ndf_cleaned.show(5)\r\nprint(f\"Number of legal rows: {legal_rows_count}\")"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql.functions import col\n\ndf \u003d df_cleaned.select(\u0027trip_distance\u0027, \u0027total_amount\u0027)\n\ncorrelation \u003d df.corr(\u0027trip_distance\u0027, \u0027total_amount\u0027)\nprint(f\"Correlation between Trip Distance and Total Fare: {correlation}\")\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\n\r\ndf \u003d df_cleaned.select(\u0027passenger_count\u0027, \u0027total_amount\u0027)\r\n\r\nfare_by_passenger_count \u003d df.groupBy(\u0027passenger_count\u0027).avg(\u0027total_amount\u0027)\r\n\r\nz.show(fare_by_passenger_count)\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\n\r\ndf \u003d df_cleaned.select(\u0027payment_type\u0027)\r\n\r\npayment_distribution \u003d df.groupBy(\u0027payment_type\u0027).count()\r\n\r\nz.show(payment_distribution)\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\n\r\ndf \u003d df_cleaned.select(\u0027PULocationID\u0027)\r\n\r\npickup_counts \u003d df.groupBy(\u0027PULocationID\u0027).count().orderBy(\u0027count\u0027, ascending\u003dFalse).limit(20)\r\n\r\nz.show(pickup_counts)\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\n\r\ndf \u003d df_cleaned.select(\u0027PULocationID\u0027, \u0027DOLocationID\u0027)\r\n\r\nroute_counts \u003d df.groupBy(\u0027PULocationID\u0027, \u0027DOLocationID\u0027).count().orderBy(\u0027count\u0027, ascending\u003dFalse).limit(20)\r\n\r\nz.show(route_counts)\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\nfrom pyspark.sql.functions import unix_timestamp\r\n\r\ndf \u003d df_cleaned.select(\u0027lpep_pickup_datetime\u0027, \u0027lpep_dropoff_datetime\u0027, \u0027total_amount\u0027)\r\n\r\ndf \u003d df.withColumn(\u0027duration_minutes\u0027, (unix_timestamp(\u0027lpep_dropoff_datetime\u0027) - unix_timestamp(\u0027lpep_pickup_datetime\u0027)) / 60)\r\n\r\nduration_fare \u003d df.select(\u0027duration_minutes\u0027, \u0027total_amount\u0027)\r\n\r\nz.show(duration_fare)\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\n\r\ndf \u003d df_cleaned.select(\u0027PULocationID\u0027, \u0027DOLocationID\u0027)\r\n\r\nlocation_counts \u003d df.groupBy(\u0027PULocationID\u0027, \u0027DOLocationID\u0027).count()\r\n\r\nz.show(location_counts)\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\nfrom pyspark.sql.functions import date_format, col, sum, desc\r\n\r\ndf \u003d df_cleaned.withColumn(\"month\", date_format(col(\"lpep_pickup_datetime\"), \"yyyy-MM\"))\r\n\r\nmonthly_revenue \u003d df.groupBy(\"month\").agg(sum(\"total_amount\").alias(\"total_revenue\"))\r\n\r\nmonthly_revenue \u003d monthly_revenue.orderBy(desc(\"month\"))\r\n\r\nz.show(monthly_revenue)"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\nfrom pyspark.sql.functions import count\r\n\r\nlocation_heatmap \u003d df_cleaned.groupBy(\"PULocationID\", \"DOLocationID\").agg(count(\"*\").alias(\"trip_count\"))\r\n\r\nlocation_heatmap \u003d location_heatmap.orderBy(col(\"trip_count\").desc())\r\n\r\nz.show(location_heatmap)\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\nfrom pyspark.sql.functions import hour, unix_timestamp\r\n\r\ndf \u003d df_cleaned.withColumn(\"trip_duration\", \r\n    (unix_timestamp(\"lpep_dropoff_datetime\") - unix_timestamp(\"lpep_pickup_datetime\")) / 60)\r\n\r\ndf \u003d df.withColumn(\"pickup_hour\", hour(\"lpep_pickup_datetime\"))\r\n\r\nhourly_duration \u003d df.groupBy(\"pickup_hour\").agg({\"trip_duration\": \"avg\"}).orderBy(\"pickup_hour\")\r\n\r\nz.show(hourly_duration)\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\nfrom pyspark.sql.functions import when\r\n\r\ndf_fraud \u003d df_cleaned.withColumn(\"fraud_flag\", when((col(\"trip_distance\") \u003c 0.1) \u0026 (col(\"total_amount\") \u003e 20), \"Overcharge\") \\\r\n    .when((col(\"trip_distance\") \u003e 100) \u0026 (col(\"total_amount\") \u003c 10), \"Undercharge\") \\\r\n    .when((col(\"tip_amount\") \u003e col(\"total_amount\") * 0.5), \"Excessive Tip\") \\\r\n    .otherwise(\"Normal\"))\r\n\r\nz.show(df_fraud.groupBy(\"fraud_flag\").count())\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\nfrom pyspark.sql.functions import unix_timestamp\r\nfrom pyspark.ml.feature import VectorAssembler\r\n\r\n# Convert date column to Unix timestamp (integer)\r\ndaily_revenue \u003d daily_revenue.withColumn(\"date_numeric\", unix_timestamp(\"date\"))\r\n\r\n# Assemble features\r\nvec_assembler \u003d VectorAssembler(inputCols\u003d[\"date_numeric\", \"total_revenue\"], outputCol\u003d\"features\")\r\ndf_features \u003d vec_assembler.transform(daily_revenue)\r\n\r\nz.show(df_features)\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\nfrom pyspark.sql.functions import hour\r\n\r\ndf \u003d df_cleaned.withColumn(\"hour_of_day\", hour(\"lpep_pickup_datetime\"))\r\n\r\nhourly_demand \u003d df.groupBy(\"hour_of_day\").count().orderBy(\"hour_of_day\")\r\n\r\nz.show(hourly_demand)\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\n\r\ndf \u003d df_cleaned.withColumn(\"price_per_mile\", col(\"total_amount\") / col(\"trip_distance\"))\r\n\r\ndistance_pricing \u003d df.groupBy(\"trip_distance\").agg({\"price_per_mile\": \"avg\"}).orderBy(\"trip_distance\")\r\n\r\nz.show(distance_pricing)\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\nfrom pyspark.sql.functions import dayofweek\r\n\r\ndf \u003d df_cleaned.withColumn(\"day_of_week\", dayofweek(\"lpep_pickup_datetime\"))\r\n\r\nweekly_demand \u003d df.groupBy(\"day_of_week\").count().orderBy(\"day_of_week\")\r\n\r\nz.show(weekly_demand)\r\n"
    }
  ]
}